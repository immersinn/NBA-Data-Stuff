+ review code, make sure ESPN scrape still works ok
	> seems to work fine, except for a few module references, etc
+ figure out where you were / what needs to be done
	> ummm…better idea: reorganize code, yo; seriously it looks like shit, ha

+ get rid of your stupid sand naming conventions, gah
	> ok nevermind, that stuff looks like total shit
+ don't store data files in Git; use DBs, yo!
+ migrate needed files over to new git with better name
+ will require developing new SQL stuff with new python package

> scraping and storing and retrieving modules are one section
> analytics is another section that isn't necessairly on GiHub, and spans both python and R most likely..

Process flow (maybe):
> have some game ids (why do we have them?)
> for each id we're going to do something with, create a game instance
	> check db to see if info about game stored
		> if yes, pull details from db
		> if no, pull details from web and write to db(s)
		> web-pull formatting must match db-read/-write format
	> methods / actions:
		> who plays in the game (team, players)
		> sequence of players on the court for entire game
		> write info about players in the game to dbs (stats, shots)

Player objects:
	> initialize with a player id
	> pull back player info as requested from dbs 


15/02/16:

need to update documentation in general (plane flight task…)

=====================================================================


Player Stats DB:
	> sql? mongo is probably easier...
	> for each game, for each player, records summary stats for player
	> pid is included --> indexed on
	> Game id for each entry --> indexed on
	> random id given to each entry

Game Summary DB:
	> teams (home, away)
	> starting players for each team
	> bench players for each team
	> game summary (text)
	{'home' : {'Name':akjlda,
		   'starters':[],
		   'bench':[],
	           'stats':{}
		  }
	'away' : etc.}



